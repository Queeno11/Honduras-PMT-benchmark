{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import r2_score, accuracy_score, fbeta_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "df = pd.read_stata(r\"D:\\World Bank\\Honduras PMT benchmark\\Data_out\\CONSOLIDADA_2023_clean.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"EDAD2\"] = df[\"EDAD\"]**2\n",
    "# oh1 = pd.get_dummies(df['CIVIL'], prefix=\"CIVIL_\", dummy_na=True)\n",
    "# oh2 = pd.get_dummies(df['CH307'], prefix=\"dis_\", dummy_na=True)\n",
    "# oh3 = pd.get_dummies(df['CH308'], prefix=\"orig_\", dummy_na=True)\n",
    "# oh4 = pd.get_dummies(df['OC609'], prefix=\"trab1_\", dummy_na=True)\n",
    "# oh5 = pd.get_dummies(df['CATEGOP'], prefix=\"trab2_\", dummy_na=True)\n",
    "# oh6 = pd.get_dummies(df['RAMAOP'], prefix=\"trab3_\", dummy_na=True)\n",
    "# oh7 = pd.get_dummies(df['OCUPAOP'], prefix=\"trab4_\", dummy_na=True)\n",
    "# oh8 = pd.get_dummies(df['DOMI'], prefix=\"domi_\", dummy_na=True)\n",
    "# oh9 = pd.get_dummies(df['SEXO'], prefix=\"genero_\", dummy_na=True)\n",
    "# oh10 = pd.get_dummies(df['ED01'], prefix=\"ed_\", dummy_na=True)\n",
    "# oh11 = pd.get_dummies(df['CA501'], prefix=\"ca501_\", dummy_na=True)\n",
    "# oh12 = pd.get_dummies(df['UR'], prefix=\"ur_\", dummy_na=True)\n",
    "# oh13 = pd.get_dummies(df['NBI'], prefix=\"nbi_\", dummy_na=True)\n",
    "# one_hots = pd.concat([oh1, oh2, oh3, oh4, oh5, oh6, oh7, oh8, oh9, oh10, oh11, oh12, oh13], axis=1)\n",
    "# one_hots_cols = one_hots.columns.to_list()\n",
    "# df = pd.concat([df, one_hots], axis=1)\n",
    "# df = pd.concat([df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales para las distintas estimaciones **\n",
    "vars_pmtoriginal = [\"Ocupacion_bien\", \"Paredes_bien\", \"Pension_bien\", \"Refri_mal\", \"Remesas_bien\", \"Aire_mal\", \"Alquileres_bien\", \"Alumbrado_bien\", \"Basura_bien\", \"Carro_mal\", \"Cocina2_bien\", \"Compu_mal\", \"Dependencia\", \"dv111\", \"Ed_diversif_bien\", \"Ed_univer_bien\", \"edad_0_5\", \"edad_15_21\", \"edad_60_120\", \"edad_6_14\", \"Estufa_mal\", \"Vivienda2_bien\", \"EqSonido_mal\", \"HaySanitario_bien\", \"Sanitario_bien\", \"Civil_mal\", \"Cocina_bien\", \"Cable_mal\", \"Hacinamiento\", \"Moto_mal\", \"Piso_mal\", \"Bici_mal\", \"Exterior_bien\", \"Dominio_1\", \"Dominio_2\", \"Dominio_3\", \"Vivienda_bien\", \"Agua2_bien\", \"Agua_bien\", \"TV_mal\", \"Telefono_mal\"]\n",
    "IPM_vars = [\"privacion_agua_h\", \"privacion_saneamiento_h\", \"privacion_cocina_h\", \"privacion_educ_h\", \"privacion_asistencia_h\", \"privacion_alfab_h\", \"privacion_elec_h\", \"privacion_piso_h\", \"privacion_techo_h\", \"privacion_segsoc_h\", \"privacion_desocup_h\", \"privacion_subemp_h\", \"privacion_ocup_h\", \"privacion_trabinf_h\", \"privacion_trabadol1_h\", \"privacion_trabadol2_h\", \"privacion_pared_h\", \"privacion_hacina_h\"]\n",
    "vars_conjunto1 = vars_pmtoriginal + IPM_vars\n",
    "\n",
    "#* Eliminamos las variables que eran muy multicolineales, eran falseables o no dependian de los hogares\n",
    "vars_conjunto2 = [\"privacion_saneamiento_h\",\"privacion_cocina_h\",\"privacion_educ_h\",\"privacion_asistencia_h\",\"privacion_alfab_h\",\"Piso_mal\",\"Paredes_bien\",\"EqSonido_mal\",\"HaySanitario_bien\",\"Sanitario_bien\",\"Cocina2_bien\",\"Hacinamiento\",\"Cable_mal\",\"Moto_mal\",\"Bici_mal\",\"Dominio_1\",\"Dominio_2\",\"Dominio_3\",\"Pension_bien\",\"Refri_mal\",\"Aire_mal\",\"Carro_mal\",\"Compu_mal\",\"dv111\",\"Ed_diversif_bien\",\"Ed_univer_bien\",\"edad_0_5\",\"edad_15_21\",\"edad_60_120\",\"edad_6_14\",\"Estufa_mal\",\"Agua2_bien\",\"Dependencia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[df.test_set==1]\n",
    "df_train = df[df.test_set==0]\n",
    "X_test = df_test[vars_conjunto1]\n",
    "y_test = df_test[\"logingreso\"]\n",
    "w_test = df_test[\"FACTOR_P\"]\n",
    "X_train = df_train[vars_conjunto1]\n",
    "y_train = df_train[\"logingreso\"]\n",
    "w_train = df_train[\"FACTOR_P\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOGAR</th>\n",
       "      <th>NPER</th>\n",
       "      <th>DOMINIO</th>\n",
       "      <th>DEPMUESTRA</th>\n",
       "      <th>COR_PRE</th>\n",
       "      <th>NUM_REC</th>\n",
       "      <th>NUM_HOG</th>\n",
       "      <th>V01</th>\n",
       "      <th>V02</th>\n",
       "      <th>V02OTRO</th>\n",
       "      <th>...</th>\n",
       "      <th>privacion_hacina_h</th>\n",
       "      <th>FACTOR_P</th>\n",
       "      <th>hogar</th>\n",
       "      <th>indice_pobreza_multi</th>\n",
       "      <th>pobreza_multidim</th>\n",
       "      <th>no_pob_multidim</th>\n",
       "      <th>log_ingreso_est</th>\n",
       "      <th>ingreso_est</th>\n",
       "      <th>random_num</th>\n",
       "      <th>test_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133 1 10384 31381</td>\n",
       "      <td>1</td>\n",
       "      <td>Ciudades Medianas</td>\n",
       "      <td>Atlantida</td>\n",
       "      <td>10384</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Casa individual</td>\n",
       "      <td>Madera al natural</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1061.000612</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.275743</td>\n",
       "      <td>531.521079</td>\n",
       "      <td>0.973354</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1257108080868131</td>\n",
       "      <td>1</td>\n",
       "      <td>Distrito Central</td>\n",
       "      <td>Francisco Morazan</td>\n",
       "      <td>80868</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Casa individual</td>\n",
       "      <td>Otro</td>\n",
       "      <td>LAMINA DE ZINC</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>493.341042</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.305543</td>\n",
       "      <td>547.598775</td>\n",
       "      <td>0.533083</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>732416160235081</td>\n",
       "      <td>1</td>\n",
       "      <td>Ciudades Pequeñas</td>\n",
       "      <td>Santa Barbara</td>\n",
       "      <td>160235</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Casa individual</td>\n",
       "      <td>Ladrillo, piedra o bloque</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1661.798779</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.451044</td>\n",
       "      <td>633.363460</td>\n",
       "      <td>0.444628</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>934405053677081</td>\n",
       "      <td>1</td>\n",
       "      <td>Ciudades Pequeñas</td>\n",
       "      <td>Cortes</td>\n",
       "      <td>53677</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Casa individual</td>\n",
       "      <td>Ladrillo, piedra o bloque</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1546.433454</td>\n",
       "      <td>1</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.474489</td>\n",
       "      <td>648.387745</td>\n",
       "      <td>0.885603</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>312406060730101</td>\n",
       "      <td>1</td>\n",
       "      <td>Ciudades Pequeñas</td>\n",
       "      <td>Choluteca</td>\n",
       "      <td>60730</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Casa individual</td>\n",
       "      <td>Ladrillo, piedra o bloque</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1876.150608</td>\n",
       "      <td>1</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.518643</td>\n",
       "      <td>677.658082</td>\n",
       "      <td>0.897054</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16389</th>\n",
       "      <td>1451515080106106</td>\n",
       "      <td>1</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Olancho</td>\n",
       "      <td>150801</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Casa individual</td>\n",
       "      <td>Adobe</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.292256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.335857</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16390</th>\n",
       "      <td>851313014502125</td>\n",
       "      <td>1</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Lempira</td>\n",
       "      <td>130145</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Casa individual</td>\n",
       "      <td>Adobe</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>379.998972</td>\n",
       "      <td>1</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.908036</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16391</th>\n",
       "      <td>417513130619051</td>\n",
       "      <td>1</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Lempira</td>\n",
       "      <td>130619</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Casa individual</td>\n",
       "      <td>Adobe</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.732757</td>\n",
       "      <td>1</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.970280</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16393</th>\n",
       "      <td>832505052880091</td>\n",
       "      <td>1</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Cortes</td>\n",
       "      <td>52880</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Casa individual</td>\n",
       "      <td>Ladrillo, piedra o bloque</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1314.773821</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.623333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16395</th>\n",
       "      <td>831505052853061</td>\n",
       "      <td>1</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Cortes</td>\n",
       "      <td>52853</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Casa individual</td>\n",
       "      <td>Ladrillo, piedra o bloque</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>751.299326</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.455658</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11489 rows × 469 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      HOGAR  NPER            DOMINIO         DEPMUESTRA  \\\n",
       "0         133 1 10384 31381     1  Ciudades Medianas          Atlantida   \n",
       "1          1257108080868131     1   Distrito Central  Francisco Morazan   \n",
       "3           732416160235081     1  Ciudades Pequeñas      Santa Barbara   \n",
       "4           934405053677081     1  Ciudades Pequeñas             Cortes   \n",
       "5           312406060730101     1  Ciudades Pequeñas          Choluteca   \n",
       "...                     ...   ...                ...                ...   \n",
       "16389      1451515080106106     1              Rural            Olancho   \n",
       "16390       851313014502125     1              Rural            Lempira   \n",
       "16391       417513130619051     1              Rural            Lempira   \n",
       "16393       832505052880091     1              Rural             Cortes   \n",
       "16395       831505052853061     1              Rural             Cortes   \n",
       "\n",
       "       COR_PRE  NUM_REC  NUM_HOG              V01                        V02  \\\n",
       "0        10384        3        1  Casa individual          Madera al natural   \n",
       "1        80868       13        1  Casa individual                       Otro   \n",
       "3       160235        8        1  Casa individual  Ladrillo, piedra o bloque   \n",
       "4        53677        8        1  Casa individual  Ladrillo, piedra o bloque   \n",
       "5        60730       10        1  Casa individual  Ladrillo, piedra o bloque   \n",
       "...        ...      ...      ...              ...                        ...   \n",
       "16389   150801        6        1  Casa individual                      Adobe   \n",
       "16390   130145        2        1  Casa individual                      Adobe   \n",
       "16391   130619        5        1  Casa individual                      Adobe   \n",
       "16393    52880        9        1  Casa individual  Ladrillo, piedra o bloque   \n",
       "16395    52853        6        1  Casa individual  Ladrillo, piedra o bloque   \n",
       "\n",
       "              V02OTRO  ... privacion_hacina_h     FACTOR_P hogar  \\\n",
       "0                      ...                1.0  1061.000612     1   \n",
       "1      LAMINA DE ZINC  ...                1.0   493.341042     1   \n",
       "3                      ...                0.0  1661.798779     1   \n",
       "4                      ...                0.0  1546.433454     1   \n",
       "5                      ...                0.0  1876.150608     1   \n",
       "...               ...  ...                ...          ...   ...   \n",
       "16389                  ...                0.0   310.292256     1   \n",
       "16390                  ...                0.0   379.998972     1   \n",
       "16391                  ...                0.0   276.732757     1   \n",
       "16393                  ...                1.0  1314.773821     1   \n",
       "16395                  ...                0.0   751.299326     1   \n",
       "\n",
       "      indice_pobreza_multi pobreza_multidim no_pob_multidim log_ingreso_est  \\\n",
       "0                 0.500000              1.0             0.0        6.275743   \n",
       "1                 0.541667              1.0             0.0        6.305543   \n",
       "3                 0.083333              0.0             1.0        6.451044   \n",
       "4                 0.416667              1.0             0.0        6.474489   \n",
       "5                 0.416667              1.0             0.0        6.518643   \n",
       "...                    ...              ...             ...             ...   \n",
       "16389             0.041667              0.0             1.0             NaN   \n",
       "16390             0.208333              0.0             1.0             NaN   \n",
       "16391             0.291667              1.0             0.0             NaN   \n",
       "16393             0.250000              0.0             1.0             NaN   \n",
       "16395             0.083333              0.0             1.0             NaN   \n",
       "\n",
       "      ingreso_est random_num test_set  \n",
       "0      531.521079   0.973354      0.0  \n",
       "1      547.598775   0.533083      0.0  \n",
       "3      633.363460   0.444628      0.0  \n",
       "4      648.387745   0.885603      0.0  \n",
       "5      677.658082   0.897054      0.0  \n",
       "...           ...        ...      ...  \n",
       "16389         NaN   0.335857      0.0  \n",
       "16390         NaN   0.908036      0.0  \n",
       "16391         NaN   0.970280      0.0  \n",
       "16393         NaN   0.623333      0.0  \n",
       "16395         NaN   0.455658      0.0  \n",
       "\n",
       "[11489 rows x 469 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_model(df, y_var, vars):\n",
    "    \"\"\" Fit a linear model to the data \"\"\"\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    df_test = df[df.test_set==1]\n",
    "    df_test = df_test[vars + [y_var, \"FACTOR_P\"]].dropna()\n",
    "    df_train = df[df.test_set==0]\n",
    "    df_train = df_train[vars + [y_var, \"FACTOR_P\"]].dropna()\n",
    "    X_test = df_test[vars]\n",
    "    y_test = df_test[y_var]\n",
    "    w_test = df_test[\"FACTOR_P\"]\n",
    "    X_train = df_train[vars]\n",
    "    y_train = df_train[y_var]\n",
    "    w_train = df_train[\"FACTOR_P\"]\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train, sample_weight=w_train)\n",
    "    y_test_preds = model.predict(X_test)\n",
    "    y_train_preds = model.predict(X_train)\n",
    "        \n",
    "    # Export dataframe with predictions\n",
    "    df_test[\"logingreso_linear\"] = y_test_preds\n",
    "    df_train[\"logingreso_linear\"] = y_train_preds\n",
    "    df_out = pd.concat([df_test, df_train])\n",
    "    df_out = df_out.sort_index()\n",
    "\n",
    "    return df_out, model, y_test_preds, y_train_preds, y_test, y_train, w_test, w_train\n",
    "    \n",
    "    \n",
    "\n",
    "def fit_xgboost_reg(df, y_var, vars, params_grid, scoring=\"neg_mean_absolute_error\"):\n",
    "    \"\"\"\n",
    "    Trains and evaluates an XGBoost regression model using a randomized grid search for hyperparameter tuning.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the dataset with both training and test sets, including features and target variables.\n",
    "        The DataFrame should have a column 'test_set' where 0 indicates training data and 1 indicates test data.\n",
    "    \n",
    "    y_var: str\n",
    "        A column name representing the variable to use as labels of the model.\n",
    "\n",
    "    vars : list\n",
    "        A list of column names representing the independent variables (features) used for training the model.\n",
    "        \n",
    "    params_grid : dict\n",
    "        Dictionary where keys are XGBoost hyperparameters and values are lists of possible values for those\n",
    "        hyperparameters. This grid is used for randomized hyperparameter tuning.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    random_search : RandomizedSearchCV object\n",
    "        The fitted RandomizedSearchCV object containing the best estimator and results from the grid search.\n",
    "        \n",
    "    Workflow:\n",
    "    ---------\n",
    "    1. Splits the input DataFrame into training and test sets based on the 'test_set' column.\n",
    "    2. Prepares the feature matrix (X) and target variable (y) for both training and test sets.\n",
    "    3. Converts the data into DMatrix format required for XGBoost.\n",
    "    4. Initializes an XGBoost regressor with the 'hist' tree method and 'cuda' for GPU acceleration.\n",
    "    5. Conducts a RandomizedSearchCV to tune hyperparameters based on a given parameter grid.\n",
    "    6. Fits the model to the training data and evaluates its performance on both the training and test sets.\n",
    "    7. Prints R-squared metrics for both training and test sets.\n",
    "    8. Returns the fitted RandomizedSearchCV object.\n",
    "    \"\"\"\n",
    "\n",
    "    df_test = df[df.test_set==1]\n",
    "    df_train = df[df.test_set==0]\n",
    "    X_test = df_test[vars]\n",
    "    y_test = df_test[y_var]\n",
    "    w_test = df_test[\"FACTOR_P\"]\n",
    "    X_train = df_train[vars]\n",
    "    y_train = df_train[y_var]\n",
    "    w_train = df_train[\"FACTOR_P\"]\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model, \n",
    "        param_distributions=params_grid, \n",
    "        n_iter=5, \n",
    "        cv=5, \n",
    "        scoring='neg_mean_absolute_error', \n",
    "        verbose=0, \n",
    "    )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    y_test_preds = random_search.best_estimator_.predict(X_test)\n",
    "    y_train_preds = random_search.best_estimator_.predict(X_train)\n",
    "    \n",
    "    # Export dataframe with predictions\n",
    "    df_test[\"logingreso_xgboost\"] = y_test_preds\n",
    "    df_train[\"logingreso_xgboost\"] = y_train_preds\n",
    "    df_out = pd.concat([df_test, df_train])\n",
    "    df_out = df_out.sort_index()\n",
    "    \n",
    "    return df_out, random_search, y_test_preds, y_train_preds, y_test, y_train, w_test, w_train\n",
    "\n",
    "def asigna_beneficios(y, weights, percentage):\n",
    "    df = pd.DataFrame({\"y\": y, \"weights\": weights})\n",
    "    df = df.sort_values(\"y\")\n",
    "\n",
    "    ranking = df[\"weights\"].cumsum()\n",
    "    ranking_pct = ranking / ranking.max()    \n",
    "    y_bool = (ranking_pct < percentage)    \n",
    "    y_bool = y_bool.sort_index()\n",
    "    \n",
    "    return y_bool\n",
    "\n",
    "def compute_metrics(y, y_preds, weights=None, percentage=.667):\n",
    "    r_test = r2_score(y, y_preds, sample_weight=weights, multioutput='uniform_average')\n",
    "    print(f\"R2 test: {r_test}\")\n",
    "\n",
    "    y_bool = (y.values < np.quantile(y_preds, percentage))\n",
    "    y_preds_bool = (y_preds < np.quantile(y_preds, percentage))\n",
    "\n",
    "    acc_test = accuracy_score(y_bool, y_preds_bool, sample_weight=weights)\n",
    "    print(f\"Accuracy Test: {acc_test}\")\n",
    "    \n",
    "    f2_test = fbeta_score(y_bool, y_preds_bool, beta=2, sample_weight=weights)\n",
    "    print(f\"F2 Test: {f2_test}\")\n",
    "    \n",
    "    return r_test, acc_test, f2_test\n",
    "\n",
    "def compute_metrics_urru(y_ur, y_preds_ur, y_ru, y_preds_ru, weights_ur=None, weights_ru=None, percentage_ru=.665468, percentage_ur=.6671753):\n",
    "\n",
    "    # Create boolean arrays for each dataset\n",
    "    y_bool_ru = asigna_beneficios(y_ru, weights_ru, percentage_ru)\n",
    "    y_preds_bool_ru = asigna_beneficios(y_preds_ru, weights_ru, percentage_ru)\n",
    "    y_bool_ur = asigna_beneficios(y_ur, weights_ur, percentage_ur)\n",
    "    y_preds_bool_ur = asigna_beneficios(y_preds_ur, weights_ur, percentage_ur)\n",
    "\n",
    "    # Create single arrays for all data\n",
    "    y_bool = pd.concat([pd.Series(y_bool_ru), pd.Series(y_bool_ur)])\n",
    "    y_preds_bool = pd.concat([pd.Series(y_preds_bool_ru), pd.Series(y_preds_bool_ur)])\n",
    "    \n",
    "    y = pd.concat([pd.Series(y_ru), pd.Series(y_ur)])\n",
    "    y_preds = pd.concat([pd.Series(y_preds_ru), pd.Series(y_preds_ur)])\n",
    "    weights = pd.concat([pd.Series(weights_ru), pd.Series(weights_ur)])\n",
    "\n",
    "    r_test = r2_score(y, y_preds, sample_weight=weights, multioutput='uniform_average')\n",
    "    print(f\"R2 test: {r_test}\")\n",
    "\n",
    "    acc_test = accuracy_score(y_bool, y_preds_bool, sample_weight=weights)\n",
    "    print(f\"Accuracy Test: {acc_test}\")\n",
    "    \n",
    "    f2_test = fbeta_score(y_bool, y_preds_bool, beta=2, sample_weight=weights)\n",
    "    print(f\"F2 Test: {f2_test}\")\n",
    "    \n",
    "    return r_test, acc_test, f2_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: vars_all\n",
      "Linear Model\n",
      "R2 test: 0.4190679871791275\n",
      "Accuracy Test: 0.7821013880461709\n",
      "F2 Test: 0.8365146038449838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:158: UserWarning: [10:29:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test[\"logingreso_xgboost\"] = y_test_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"logingreso_xgboost\"] = y_test_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train[\"logingreso_xgboost\"] = y_train_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"logingreso_xgboost\"] = y_train_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test[\"logingreso_xgboost\"] = y_test_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"logingreso_xgboost\"] = y_test_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train[\"logingreso_xgboost\"] = y_train_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"logingreso_xgboost\"] = y_train_preds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model\n",
      "R2 test: 0.4356632387235657\n",
      "Accuracy Test: 0.7757733298743447\n",
      "F2 Test: 0.8316333175174002\n",
      "Running model: vars_min\n",
      "Linear Model\n",
      "R2 test: 0.39901290234450326\n",
      "Accuracy Test: 0.7667862756314439\n",
      "F2 Test: 0.8248207357898657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test[\"logingreso_xgboost\"] = y_test_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"logingreso_xgboost\"] = y_test_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train[\"logingreso_xgboost\"] = y_train_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"logingreso_xgboost\"] = y_train_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test[\"logingreso_xgboost\"] = y_test_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"logingreso_xgboost\"] = y_test_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:98: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train[\"logingreso_xgboost\"] = y_train_preds\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_20260\\1471250287.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"logingreso_xgboost\"] = y_train_preds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model\n",
      "R2 test: 0.3977111773081161\n",
      "Accuracy Test: 0.7618160790841089\n",
      "F2 Test: 0.8212858949687621\n"
     ]
    }
   ],
   "source": [
    "params_grid = {\n",
    "    'learning_rate': stats.uniform(0.005, 0.1),\n",
    "    'reg_alpha': stats.uniform(0, 0.1),\n",
    "    'reg_lambda': stats.uniform(0.7, 0.3),\n",
    "    'gamma':stats.uniform(0.1, 0.5),\n",
    "    'max_depth': stats.randint(5, 30),\n",
    "    'min_child_weight': stats.randint(5, 30),\n",
    "    'n_estimators': stats.randint(50, 500),\n",
    "    'colsample_bytree':stats.uniform(0.5, 0.5),\n",
    "    'subsample': stats.uniform(0.5, .25),\n",
    "}\n",
    "\n",
    "casos = {\n",
    "    \"vars_all\": {\n",
    "        \"df\":df, \n",
    "        \"vars\": vars_conjunto1,\n",
    "    },\n",
    "    \"vars_min\": {\n",
    "        \"df\":df, \n",
    "        \"vars\": vars_conjunto2,\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "resultados = {}\n",
    "for name, params in casos.items():\n",
    "    print(f\"Running model: {name}\")\n",
    "    df_model, vars = params.values()\n",
    "    # # Nacional\n",
    "    # results, y_test_preds, y_train_preds, y_test, y_train = fit_xgboost_reg(df_model, \"logingreso\", vars, params_grid)\n",
    "    # r2_test, acc_test = compute_metrics(y_test, y_test_preds)\n",
    "    # resultados[name + \"_reg_nac\"] = {}\n",
    "    # resultados[name + \"_reg_nac\"][\"df\"] = df_model\n",
    "    # resultados[name + \"_reg_nac\"][\"results\"] = results\n",
    "    # resultados[name + \"_reg_nac\"][\"best_params\"] = results.best_params_\n",
    "    # resultados[name + \"_reg_nac\"][\"r_test\"]  = r2_test\n",
    "    # resultados[name + \"_reg_nac\"][\"y_test_preds\"] = y_test_preds\n",
    "\n",
    "    # Modelo lineal (para garantizar que esté todo ok)\n",
    "    df_out_ru, model_ru, y_test_preds_ru, y_train_preds_ru, y_test_ru, y_train_ru, w_test_ru, w_train_ru = fit_linear_model(df_model[df_model[\"UR\"]==\"Rural\"], \"logingreso\", vars)\n",
    "    df_out_ur, model_ur, y_test_preds_ur, y_train_preds_ur, y_test_ur, y_train_ur, w_test_ur, w_train_ur = fit_linear_model(df_model[df_model[\"UR\"]==\"Urbana\"], \"logingreso\", vars)\n",
    "    print(\"Linear Model\")\n",
    "    r2_test, acc_test, f2_test = compute_metrics_urru(y_test_ur, y_test_preds_ur, y_test_ru, y_test_preds_ru, weights_ur=w_test_ur, weights_ru=w_test_ru)\n",
    "    df_out = pd.concat([df_out_ru, df_out_ur]).sort_index()\n",
    "    df_out = df_out[[\"logingreso_linear\"]].join(df_model[[\"HOGAR\", \"YPERHG\", \"TOTPER\", \"FACTOR\", \"UR\", \"indice_pobreza_multi\", \"pobreza\", \"pobreza_ext\"]])\n",
    "    resultados[name + \"_linreg_urru\"] = {}\n",
    "    resultados[name + \"_linreg_urru\"][\"df\"] = df_out\n",
    "    resultados[name + \"_linreg_urru\"][\"model\"] = (model_ru, model_ur)\n",
    "    resultados[name + \"_linreg_urru\"][\"best_params\"] = {\"Rural\": (model_ru.intercept_, model_ru.coef_), \"Urbana\": (model_ur.intercept_, model_ur.coef_)}\n",
    "    resultados[name + \"_linreg_urru\"][\"r_test\"]  = r2_test\n",
    "    resultados[name + \"_linreg_urru\"][\"f2_test\"] = f2_test\n",
    "    \n",
    "    # Urbano y Rural estimados por separado\n",
    "    df_out_ru, model_ru, y_test_preds_ru, y_train_preds_ru, y_test_ru, y_train_ru, w_test_ru, w_train_ru = fit_xgboost_reg(df_model[df_model[\"UR\"]==\"Rural\"], \"logingreso\", vars, params_grid)\n",
    "    df_out_ur, model_ur, y_test_preds_ur, y_train_preds_ur, y_test_ur, y_train_ur, w_test_ur, w_train_ur = fit_xgboost_reg(df_model[df_model[\"UR\"]==\"Urbana\"], \"logingreso\", vars, params_grid)\n",
    "    print(\"XGBoost Model\")\n",
    "    r2_test, acc_test, f2_test = compute_metrics_urru(y_test_ur, y_test_preds_ur, y_test_ru, y_test_preds_ru, weights_ur=w_test_ur, weights_ru=w_test_ru)\n",
    "    df_out = pd.concat([df_out_ru, df_out_ur]).sort_index()\n",
    "    df_out = df_out[[\"logingreso_xgboost\"]].join(df_model[[\"HOGAR\", \"YPERHG\", \"TOTPER\", \"FACTOR\", \"UR\", \"indice_pobreza_multi\", \"pobreza\", \"pobreza_ext\"]])\n",
    "\n",
    "    resultados[name + \"_xgboost_urru\"] = {}\n",
    "    resultados[name + \"_xgboost_urru\"][\"df\"] = df_out\n",
    "    resultados[name + \"_xgboost_urru\"][\"model\"] = (model_ru, model_ur)\n",
    "    resultados[name + \"_xgboost_urru\"][\"best_params\"] = {\"Rural\": model_ru.best_params_, \"Urban\": model_ur.best_params_}\n",
    "    resultados[name + \"_xgboost_urru\"][\"r_test\"]  = r2_test\n",
    "    resultados[name + \"_xgboost_urru\"][\"f2_test\"] = f2_test\n",
    "\n",
    "    # results, y_test_preds, r_test, r_train, cm = fit_xgboost_cla(df_model, \"pobreza\", vars, params_grid)\n",
    "    # resultados[name + \"_cla\"] = {}\n",
    "    # resultados[name + \"_cla\"][\"df\"] = df_model\n",
    "    # resultados[name + \"_cla\"][\"results\"] = results\n",
    "    # resultados[name + \"_cla\"][\"best_params\"] = results.best_params_\n",
    "    # resultados[name + \"_cla\"][\"r_test\"]  = r_test\n",
    "    # resultados[name + \"_cla\"][\"r_train\"] = r_train\n",
    "    # resultados[name + \"_cla\"][\"confusion_matrix\"] = cm\n",
    "    # resultados[name + \"_cla\"][\"y_test_preds\"] = y_test_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados[\"vars_min_xgboost_urru\"][\"df\"].to_stata(r\"D:\\World Bank\\Honduras PMT benchmark\\Data_out\\Predicts_XGBoost.dta\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

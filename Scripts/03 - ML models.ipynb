{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import r2_score, accuracy_score, fbeta_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "df = pd.read_stata(r\"D:\\World Bank\\Honduras PMT benchmark\\Data_out\\CONSOLIDADA_2023_clean.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"EDAD2\"] = df[\"EDAD\"]**2\n",
    "# oh1 = pd.get_dummies(df['CIVIL'], prefix=\"CIVIL_\", dummy_na=True)\n",
    "# oh2 = pd.get_dummies(df['CH307'], prefix=\"dis_\", dummy_na=True)\n",
    "# oh3 = pd.get_dummies(df['CH308'], prefix=\"orig_\", dummy_na=True)\n",
    "# oh4 = pd.get_dummies(df['OC609'], prefix=\"trab1_\", dummy_na=True)\n",
    "# oh5 = pd.get_dummies(df['CATEGOP'], prefix=\"trab2_\", dummy_na=True)\n",
    "# oh6 = pd.get_dummies(df['RAMAOP'], prefix=\"trab3_\", dummy_na=True)\n",
    "# oh7 = pd.get_dummies(df['OCUPAOP'], prefix=\"trab4_\", dummy_na=True)\n",
    "# oh8 = pd.get_dummies(df['DOMI'], prefix=\"domi_\", dummy_na=True)\n",
    "# oh9 = pd.get_dummies(df['SEXO'], prefix=\"genero_\", dummy_na=True)\n",
    "# oh10 = pd.get_dummies(df['ED01'], prefix=\"ed_\", dummy_na=True)\n",
    "# oh11 = pd.get_dummies(df['CA501'], prefix=\"ca501_\", dummy_na=True)\n",
    "# oh12 = pd.get_dummies(df['UR'], prefix=\"ur_\", dummy_na=True)\n",
    "# oh13 = pd.get_dummies(df['NBI'], prefix=\"nbi_\", dummy_na=True)\n",
    "# one_hots = pd.concat([oh1, oh2, oh3, oh4, oh5, oh6, oh7, oh8, oh9, oh10, oh11, oh12, oh13], axis=1)\n",
    "# one_hots_cols = one_hots.columns.to_list()\n",
    "# df = pd.concat([df, one_hots], axis=1)\n",
    "# df = pd.concat([df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales para las distintas estimaciones **\n",
    "vars_pmtoriginal = [\"Ocupacion_bien\", \"Paredes_bien\", \"Pension_bien\", \"Refri_mal\", \"Remesas_bien\", \"Aire_mal\", \"Alquileres_bien\", \"Alumbrado_bien\", \"Basura_bien\", \"Carro_mal\", \"Cocina2_bien\", \"Compu_mal\", \"Dependencia\", \"dv111\", \"Ed_diversif_bien\", \"Ed_univer_bien\", \"edad_0_5\", \"edad_15_21\", \"edad_60_120\", \"edad_6_14\", \"Estufa_mal\", \"Vivienda2_bien\", \"EqSonido_mal\", \"HaySanitario_bien\", \"Sanitario_bien\", \"Civil_mal\", \"Cocina_bien\", \"Cable_mal\", \"Hacinamiento\", \"Moto_mal\", \"Piso_mal\", \"Bici_mal\", \"Exterior_bien\", \"Dominio_1\", \"Dominio_2\", \"Dominio_3\", \"Vivienda_bien\", \"Agua2_bien\", \"Agua_bien\", \"TV_mal\", \"Telefono_mal\"]\n",
    "IPM_vars = [\"privacion_agua_h\", \"privacion_saneamiento_h\", \"privacion_cocina_h\", \"privacion_educ_h\", \"privacion_asistencia_h\", \"privacion_alfab_h\", \"privacion_elec_h\", \"privacion_piso_h\", \"privacion_techo_h\", \"privacion_segsoc_h\", \"privacion_desocup_h\", \"privacion_subemp_h\", \"privacion_ocup_h\", \"privacion_trabinf_h\", \"privacion_trabadol1_h\", \"privacion_trabadol2_h\", \"privacion_pared_h\", \"privacion_hacina_h\"]\n",
    "vars_conjunto1 = vars_pmtoriginal + IPM_vars\n",
    "\n",
    "#* Eliminamos las variables que eran muy multicolineales, eran falseables o no dependian de los hogares\n",
    "vars_conjunto2 = [\"privacion_saneamiento_h\",\"privacion_cocina_h\",\"privacion_educ_h\",\"privacion_asistencia_h\",\"privacion_alfab_h\",\"Piso_mal\",\"Paredes_bien\",\"EqSonido_mal\",\"HaySanitario_bien\",\"Sanitario_bien\",\"Cocina2_bien\",\"Hacinamiento\",\"Cable_mal\",\"Moto_mal\",\"Bici_mal\",\"Dominio_1\",\"Dominio_2\",\"Dominio_3\",\"Pension_bien\",\"Refri_mal\",\"Aire_mal\",\"Carro_mal\",\"Compu_mal\",\"dv111\",\"Ed_diversif_bien\",\"Ed_univer_bien\",\"edad_0_5\",\"edad_15_21\",\"edad_60_120\",\"edad_6_14\",\"Estufa_mal\",\"Agua2_bien\",\"Dependencia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_model(df, y_var, vars):\n",
    "    \"\"\" Fit a linear model to the data \"\"\"\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    df_test = df[df.test_set==1]\n",
    "    df_test = df_test[vars + [y_var, \"FACTOR_P\"]].dropna()\n",
    "    df_train = df[df.test_set==0]\n",
    "    df_train = df_train[vars + [y_var, \"FACTOR_P\"]].dropna()\n",
    "    X_test = df_test[vars]\n",
    "    y_test = df_test[y_var]\n",
    "    w_test = df_test[\"FACTOR_P\"]\n",
    "    X_train = df_train[vars]\n",
    "    y_train = df_train[y_var]\n",
    "    w_train = df_train[\"FACTOR_P\"]\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train, sample_weight=w_train)\n",
    "    y_test_preds = model.predict(X_test)\n",
    "    y_train_preds = model.predict(X_train)\n",
    "        \n",
    "    return model, y_test_preds, y_train_preds, y_test, y_train, w_test, w_train\n",
    "    \n",
    "    \n",
    "\n",
    "def fit_xgboost_reg(df, y_var, vars, params_grid, scoring=\"neg_mean_absolute_error\"):\n",
    "    \"\"\"\n",
    "    Trains and evaluates an XGBoost regression model using a randomized grid search for hyperparameter tuning.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the dataset with both training and test sets, including features and target variables.\n",
    "        The DataFrame should have a column 'test_set' where 0 indicates training data and 1 indicates test data.\n",
    "    \n",
    "    y_var: str\n",
    "        A column name representing the variable to use as labels of the model.\n",
    "\n",
    "    vars : list\n",
    "        A list of column names representing the independent variables (features) used for training the model.\n",
    "        \n",
    "    params_grid : dict\n",
    "        Dictionary where keys are XGBoost hyperparameters and values are lists of possible values for those\n",
    "        hyperparameters. This grid is used for randomized hyperparameter tuning.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    random_search : RandomizedSearchCV object\n",
    "        The fitted RandomizedSearchCV object containing the best estimator and results from the grid search.\n",
    "        \n",
    "    Workflow:\n",
    "    ---------\n",
    "    1. Splits the input DataFrame into training and test sets based on the 'test_set' column.\n",
    "    2. Prepares the feature matrix (X) and target variable (y) for both training and test sets.\n",
    "    3. Converts the data into DMatrix format required for XGBoost.\n",
    "    4. Initializes an XGBoost regressor with the 'hist' tree method and 'cuda' for GPU acceleration.\n",
    "    5. Conducts a RandomizedSearchCV to tune hyperparameters based on a given parameter grid.\n",
    "    6. Fits the model to the training data and evaluates its performance on both the training and test sets.\n",
    "    7. Prints R-squared metrics for both training and test sets.\n",
    "    8. Returns the fitted RandomizedSearchCV object.\n",
    "    \"\"\"\n",
    "\n",
    "    df_test = df[df.test_set==1]\n",
    "    df_train = df[df.test_set==0]\n",
    "    X_test = df_test[vars]\n",
    "    y_test = df_test[y_var]\n",
    "    w_test = df_test[\"FACTOR_P\"]\n",
    "    X_train = df_train[vars]\n",
    "    y_train = df_train[y_var]\n",
    "    w_train = df_train[\"FACTOR_P\"]\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model, \n",
    "        param_distributions=params_grid, \n",
    "        n_iter=5, \n",
    "        cv=5, \n",
    "        scoring='neg_mean_absolute_error', \n",
    "        verbose=0, \n",
    "    )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    y_test_preds = random_search.best_estimator_.predict(X_test)\n",
    "    y_train_preds = random_search.best_estimator_.predict(X_train)\n",
    "    \n",
    "    return random_search, y_test_preds, y_train_preds, y_test, y_train, w_test, w_train\n",
    "\n",
    "def asigna_beneficios(y, weights, percentage):\n",
    "    df = pd.DataFrame({\"y\": y, \"weights\": weights})\n",
    "    df = df.sort_values(\"y\")\n",
    "\n",
    "    ranking = df[\"weights\"].cumsum()\n",
    "    ranking_pct = ranking / ranking.max()    \n",
    "    y_bool = (ranking_pct < percentage)    \n",
    "    y_bool = y_bool.sort_index()\n",
    "    \n",
    "    return y_bool\n",
    "\n",
    "def compute_metrics(y, y_preds, weights=None, percentage=.667):\n",
    "    r_test = r2_score(y, y_preds, sample_weight=weights, multioutput='uniform_average')\n",
    "    print(f\"R2 test: {r_test}\")\n",
    "\n",
    "    y_bool = (y.values < np.quantile(y_preds, percentage))\n",
    "    y_preds_bool = (y_preds < np.quantile(y_preds, percentage))\n",
    "\n",
    "    acc_test = accuracy_score(y_bool, y_preds_bool, sample_weight=weights)\n",
    "    print(f\"Accuracy Test: {acc_test}\")\n",
    "    \n",
    "    f2_test = fbeta_score(y_bool, y_preds_bool, beta=2, sample_weight=weights)\n",
    "    print(f\"F2 Test: {f2_test}\")\n",
    "    \n",
    "    return r_test, acc_test, f2_test\n",
    "\n",
    "def compute_metrics_urru(y_ur, y_preds_ur, y_ru, y_preds_ru, weights_ur=None, weights_ru=None, percentage_ru=.665468, percentage_ur=.6671753):\n",
    "\n",
    "    # Create boolean arrays for each dataset\n",
    "    y_bool_ru = asigna_beneficios(y_ru, weights_ru, percentage_ru)\n",
    "    y_preds_bool_ru = asigna_beneficios(y_preds_ru, weights_ru, percentage_ru)\n",
    "    y_bool_ur = asigna_beneficios(y_ur, weights_ur, percentage_ur)\n",
    "    y_preds_bool_ur = asigna_beneficios(y_preds_ur, weights_ur, percentage_ur)\n",
    "\n",
    "    # Create single arrays for all data\n",
    "    y_bool = pd.concat([pd.Series(y_bool_ru), pd.Series(y_bool_ur)])\n",
    "    y_preds_bool = pd.concat([pd.Series(y_preds_bool_ru), pd.Series(y_preds_bool_ur)])\n",
    "    \n",
    "    y = pd.concat([pd.Series(y_ru), pd.Series(y_ur)])\n",
    "    y_preds = pd.concat([pd.Series(y_preds_ru), pd.Series(y_preds_ur)])\n",
    "    weights = pd.concat([pd.Series(weights_ru), pd.Series(weights_ur)])\n",
    "\n",
    "    r_test = r2_score(y, y_preds, sample_weight=weights, multioutput='uniform_average')\n",
    "    print(f\"R2 test: {r_test}\")\n",
    "\n",
    "    acc_test = accuracy_score(y_bool, y_preds_bool, sample_weight=weights)\n",
    "    print(f\"Accuracy Test: {acc_test}\")\n",
    "    \n",
    "    f2_test = fbeta_score(y_bool, y_preds_bool, beta=2, sample_weight=weights)\n",
    "    print(f\"F2 Test: {f2_test}\")\n",
    "    \n",
    "    return r_test, acc_test, f2_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: vars_all\n",
      "Linear Model\n",
      "R2 test: 0.423916220664978\n",
      "Accuracy Test: 0.7848847679259812\n",
      "F2 Test: 0.8385454430232406\n",
      "XGBoost Model\n",
      "R2 test: 0.4154311418533325\n",
      "Accuracy Test: 0.7740522758856013\n",
      "F2 Test: 0.8303840799390814\n",
      "Running model: vars_min\n",
      "Linear Model\n",
      "R2 test: 0.3940904140472412\n",
      "Accuracy Test: 0.7655498977479872\n",
      "F2 Test: 0.8239218090847\n",
      "XGBoost Model\n",
      "R2 test: 0.38318902254104614\n",
      "Accuracy Test: 0.7627021727585709\n",
      "F2 Test: 0.8214118845292081\n"
     ]
    }
   ],
   "source": [
    "params_grid = {\n",
    "    'learning_rate': stats.uniform(0.005, 0.1),\n",
    "    'reg_alpha': stats.uniform(0, 0.1),\n",
    "    'reg_lambda': stats.uniform(0.7, 0.3),\n",
    "    'gamma':stats.uniform(0.1, 0.5),\n",
    "    'max_depth': stats.randint(5, 30),\n",
    "    'min_child_weight': stats.randint(5, 30),\n",
    "    'n_estimators': stats.randint(50, 500),\n",
    "    'colsample_bytree':stats.uniform(0.5, 0.5),\n",
    "    'subsample': stats.uniform(0.5, .25),\n",
    "}\n",
    "\n",
    "casos = {\n",
    "    \"vars_all\": {\n",
    "        \"df\":df, \n",
    "        \"vars\": vars_conjunto1,\n",
    "    },\n",
    "    \"vars_min\": {\n",
    "        \"df\":df, \n",
    "        \"vars\": vars_conjunto2,\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "resultados = {}\n",
    "for name, params in casos.items():\n",
    "    print(f\"Running model: {name}\")\n",
    "    df_model, vars = params.values()\n",
    "    # # Nacional\n",
    "    # results, y_test_preds, y_train_preds, y_test, y_train = fit_xgboost_reg(df_model, \"logingreso\", vars, params_grid)\n",
    "    # r2_test, acc_test = compute_metrics(y_test, y_test_preds)\n",
    "    # resultados[name + \"_reg_nac\"] = {}\n",
    "    # resultados[name + \"_reg_nac\"][\"df\"] = df_model\n",
    "    # resultados[name + \"_reg_nac\"][\"results\"] = results\n",
    "    # resultados[name + \"_reg_nac\"][\"best_params\"] = results.best_params_\n",
    "    # resultados[name + \"_reg_nac\"][\"r_test\"]  = r2_test\n",
    "    # resultados[name + \"_reg_nac\"][\"y_test_preds\"] = y_test_preds\n",
    "\n",
    "    # Modelo lineal (para garantizar que esté todo ok)\n",
    "    model_ru, y_test_preds_ru, y_train_preds_ru, y_test_ru, y_train_ru, w_test_ru, w_train_ru = fit_linear_model(df_model[df_model[\"UR\"]==\"Rural\"], \"logingreso\", vars)\n",
    "    model_ur, y_test_preds_ur, y_train_preds_ur, y_test_ur, y_train_ur, w_test_ur, w_train_ur = fit_linear_model(df_model[df_model[\"UR\"]==\"Urbana\"], \"logingreso\", vars)\n",
    "    print(\"Linear Model\")\n",
    "    r2_test, acc_test, f2_test = compute_metrics_urru(y_test_ur, y_test_preds_ur, y_test_ru, y_test_preds_ru, weights_ur=w_test_ur, weights_ru=w_test_ru)\n",
    "    resultados[name + \"_linreg_urru\"] = {}\n",
    "    resultados[name + \"_linreg_urru\"][\"df\"] = df_model\n",
    "    resultados[name + \"_linreg_urru\"][\"model\"] = (model_ru, model_ur)\n",
    "    resultados[name + \"_linreg_urru\"][\"best_params\"] = {\"Rural\": (model_ru.intercept_, model_ru.coef_), \"Urbana\": (model_ur.intercept_, model_ur.coef_)}\n",
    "    resultados[name + \"_linreg_urru\"][\"r_test\"]  = r2_test\n",
    "    resultados[name + \"_linreg_urru\"][\"f2_test\"] = f2_test\n",
    "    \n",
    "    # Urbano y Rural estimados por separado\n",
    "    model_ru, y_test_preds_ru, y_train_preds_ru, y_test_ru, y_train_ru, w_test_ru, w_train_ru = fit_xgboost_reg(df_model[df_model[\"UR\"]==\"Rural\"], \"logingreso\", vars, params_grid)\n",
    "    model_ur, y_test_preds_ur, y_train_preds_ur, y_test_ur, y_train_ur, w_test_ur, w_train_ur = fit_xgboost_reg(df_model[df_model[\"UR\"]==\"Urbana\"], \"logingreso\", vars, params_grid)\n",
    "    print(\"XGBoost Model\")\n",
    "    r2_test, acc_test, f2_test = compute_metrics_urru(y_test_ur, y_test_preds_ur, y_test_ru, y_test_preds_ru, weights_ur=w_test_ur, weights_ru=w_test_ru)\n",
    "    resultados[name + \"_xgboost_urru\"] = {}\n",
    "    resultados[name + \"_xgboost_urru\"][\"df\"] = df_model\n",
    "    resultados[name + \"_xgboost_urru\"][\"model\"] = (model_ru, model_ur)\n",
    "    resultados[name + \"_xgboost_urru\"][\"best_params\"] = {\"Rural\": model_ru.best_params_, \"Urban\": model_ur.best_params_}\n",
    "    resultados[name + \"_xgboost_urru\"][\"r_test\"]  = r2_test\n",
    "    resultados[name + \"_xgboost_urru\"][\"f2_test\"] = f2_test\n",
    "\n",
    "    # results, y_test_preds, r_test, r_train, cm = fit_xgboost_cla(df_model, \"pobreza\", vars, params_grid)\n",
    "    # resultados[name + \"_cla\"] = {}\n",
    "    # resultados[name + \"_cla\"][\"df\"] = df_model\n",
    "    # resultados[name + \"_cla\"][\"results\"] = results\n",
    "    # resultados[name + \"_cla\"][\"best_params\"] = results.best_params_\n",
    "    # resultados[name + \"_cla\"][\"r_test\"]  = r_test\n",
    "    # resultados[name + \"_cla\"][\"r_train\"] = r_train\n",
    "    # resultados[name + \"_cla\"][\"confusion_matrix\"] = cm\n",
    "    # resultados[name + \"_cla\"][\"y_test_preds\"] = y_test_preds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

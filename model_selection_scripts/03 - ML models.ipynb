{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import r2_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "df = pd.read_stata(r\"D:\\World Bank\\Honduras PMT benchmark\\Data_out\\CONSOLIDADA_2023_clean.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_2356\\2180309289.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"logingreso_tr\"] = stats.mstats.winsorize(df[\"logingreso\"], limits=(0.1, 0.1))\n"
     ]
    }
   ],
   "source": [
    "df[\"logingreso_tr\"] = stats.mstats.winsorize(df[\"logingreso\"], limits=(0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_2356\\931429008.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"EDAD2\"] = df[\"EDAD\"]**2\n"
     ]
    }
   ],
   "source": [
    "df[\"EDAD2\"] = df[\"EDAD\"]**2\n",
    "oh1 = pd.get_dummies(df['CIVIL'], prefix=\"CIVIL_\", dummy_na=True)\n",
    "oh2 = pd.get_dummies(df['CH307'], prefix=\"dis_\", dummy_na=True)\n",
    "oh3 = pd.get_dummies(df['CH308'], prefix=\"orig_\", dummy_na=True)\n",
    "oh4 = pd.get_dummies(df['OC609'], prefix=\"trab1_\", dummy_na=True)\n",
    "oh5 = pd.get_dummies(df['CATEGOP'], prefix=\"trab2_\", dummy_na=True)\n",
    "oh6 = pd.get_dummies(df['RAMAOP'], prefix=\"trab3_\", dummy_na=True)\n",
    "oh7 = pd.get_dummies(df['OCUPAOP'], prefix=\"trab4_\", dummy_na=True)\n",
    "oh8 = pd.get_dummies(df['DOMI'], prefix=\"domi_\", dummy_na=True)\n",
    "oh9 = pd.get_dummies(df['SEXO'], prefix=\"genero_\", dummy_na=True)\n",
    "oh10 = pd.get_dummies(df['ED01'], prefix=\"ed_\", dummy_na=True)\n",
    "oh11 = pd.get_dummies(df['CA501'], prefix=\"ca501_\", dummy_na=True)\n",
    "oh12 = pd.get_dummies(df['UR'], prefix=\"ur_\", dummy_na=True)\n",
    "oh13 = pd.get_dummies(df['NBI'], prefix=\"nbi_\", dummy_na=True)\n",
    "one_hots = pd.concat([oh1, oh2, oh3, oh4, oh5, oh6, oh7, oh8, oh9, oh10, oh11, oh12, oh13], axis=1)\n",
    "one_hots_cols = one_hots.columns.to_list()\n",
    "df = pd.concat([df, one_hots], axis=1)\n",
    "df = pd.concat([df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_PMT_nonoise = [\n",
    "    \"Ocupacion_bien\", \"Paredes_bien\", \"Pension_bien\", \"Refri_mal\",  \n",
    "    \"Aire_mal\",  \"Carro_mal\", \"Cocina2_bien\", \"Compu_mal\", \"Dependencia\", \"dv111\", \"Ed_diversif_bien\", \n",
    "    \"Ed_univer_bien\", \"edad_0_5\", \"edad_15_21\", \"edad_60_120\", \"edad_6_14\",  \"EqSonido_mal\",\n",
    "    \"Estufa_mal\", \"Vivienda2_bien\",  \"Alumbrado_bien\", \"Basura_bien\", \"HaySanitario_bien\", \"Vivienda_bien\", \"Agua2_bien\", \"Sanitario_bien\", \n",
    "    \"Agua_bien\", \"Cocina_bien\", \"Cable_mal\", \"Radio_mal\", \"Ed_basica_bien\", \"Hacinamiento\", \"TV_mal\", \"Telefono_mal\", \"Moto_mal\", \"Piso_mal\", \"Bici_mal\", \"Exterior_bien\", \"Dominio_1\", \"Dominio_2\", \"Dominio_3\", \"dv112\"]\n",
    "vars_PMT_noise = [\"Alquileres_bien\", \"Remesas_bien\", \"Civil_mal\"]\n",
    "vars_IPM = [\"privacion_agua_h\", \"privacion_saneamiento_h\", \"privacion_cocina_h\", \"privacion_educ_h\", \"privacion_asistencia_h\", \"privacion_alfab_h\", \"privacion_segsoc_h\", \"privacion_desocup_h\", \"privacion_subemp_h\", \"privacion_ocup_h\", \"privacion_trabinf_h\", \"privacion_trabadol1_h\", \"privacion_trabadol2_h\", \"privacion_elec_h\", \"privacion_piso_h\", \"privacion_techo_h\", \"privacion_pared_h\", \"privacion_hacina_h\"]\n",
    "vars_Nico = [\n",
    "    \"H01_1\",\"H01_2\",\"H01_3\",\"H01_4\",\"H01_5\",\"H01_6\",\"H01_7\",\"H01_8\",\"H01_9\",\"H01_10\",\"H01_11\",\"H01_12\", # assets\n",
    "    \"EDAD\", \"EDAD2\",\n",
    "    \"OIH01_LPS\", \"OIH02_LPS\", \"OIH04\", \"OIH06_LPS\", \"OIH13\", \"OIH14\", \"OIH15\", \"OIH16\", \"OIH17\", \"OIH18\", \"OIH19_LPS\", \"OIH19_LPS_ESP\"\n",
    "    ] + one_hots_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: vars_all_outliers_min\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:158: UserWarning: [02:49:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 test: 0.40278708934783936\n",
      "R2 train: 0.49397003650665283\n",
      "Confusion Matrix: \n",
      "[[0.28046812 0.10875706]\n",
      " [0.10673931 0.50403551]]\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test: 0.7445520581113801\n",
      "Accuracy train: 0.737656209036092\n",
      "Confusion Matrix: \n",
      "[[0.17171106 0.20641646]\n",
      " [0.04903148 0.572841  ]]\n",
      "Running model: vars_all_outliers_all\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "R2 test: 0.4414503574371338\n",
      "R2 train: 0.5915718078613281\n",
      "Confusion Matrix: \n",
      "[[0.28531073 0.10391445]\n",
      " [0.10189669 0.50887813]]\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Accuracy test: 0.7296206618240516\n",
      "Accuracy train: 0.7371318710128463\n",
      "Confusion Matrix: \n",
      "[[0.13478612 0.2433414 ]\n",
      " [0.02703793 0.59483454]]\n",
      "Running model: vars_all_outliers_30\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 test: 0.52760249376297\n",
      "R2 train: 0.6755373477935791\n",
      "Confusion Matrix: \n",
      "[[0.28649101 0.10079465]\n",
      " [0.10079465 0.5119197 ]]\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Accuracy test: 0.7534504391468005\n",
      "Accuracy train: 0.7590830841714233\n",
      "Confusion Matrix: \n",
      "[[0.18987871 0.20179841]\n",
      " [0.04475115 0.56357173]]\n",
      "Running model: vars_allPMT_outliers_all\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "R2 test: 0.38067513704299927\n",
      "R2 train: 0.48366373777389526\n",
      "Confusion Matrix: \n",
      "[[0.27784504 0.11138015]\n",
      " [0.10936239 0.50141243]]\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test: 0.7296206618240516\n",
      "Accuracy train: 0.7254216551603601\n",
      "Confusion Matrix: \n",
      "[[0.15718321 0.22094431]\n",
      " [0.04943503 0.57243745]]\n",
      "Running model: vars_allPMT_outliers_30\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 test: 0.47397422790527344\n",
      "R2 train: 0.6047360897064209\n",
      "Confusion Matrix: \n",
      "[[0.27624425 0.11104141]\n",
      " [0.11104141 0.50167294]]\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Accuracy test: 0.7214554579673776\n",
      "Accuracy train: 0.7271903596991937\n",
      "Confusion Matrix: \n",
      "[[0.16687578 0.22480134]\n",
      " [0.0537432  0.55457967]]\n",
      "Running model: vars_allPMT-IPM_outliers_all\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 test: 0.4095526933670044\n",
      "R2 train: 0.49551230669021606\n",
      "Confusion Matrix: \n",
      "[[0.2806699  0.10855529]\n",
      " [0.10653753 0.50423729]]\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Accuracy test: 0.7322437449556094\n",
      "Accuracy train: 0.7303154767106528\n",
      "Confusion Matrix: \n",
      "[[0.14608555 0.23204197]\n",
      " [0.03571429 0.58615819]]\n",
      "Running model: vars_allPMT-IPM_outliers_30\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "R2 test: 0.5062565803527832\n",
      "R2 train: 0.6103631854057312\n",
      "Confusion Matrix: \n",
      "[[0.28230866 0.104977  ]\n",
      " [0.104977   0.50773735]]\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Accuracy test: 0.732956921790046\n",
      "Accuracy train: 0.7251970644196792\n",
      "Confusion Matrix: \n",
      "[[0.16520284 0.22647428]\n",
      " [0.0405688  0.56775408]]\n",
      "Running model: vars_allPMTmin_outliers_all\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "R2 test: 0.3745834231376648\n",
      "R2 train: 0.4776254892349243\n",
      "Confusion Matrix: \n",
      "[[0.27623083 0.11299435]\n",
      " [0.11097659 0.49979822]]\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Accuracy test: 0.7140839386602098\n",
      "Accuracy train: 0.7167700777768068\n",
      "Confusion Matrix: \n",
      "[[0.13418079 0.24394673]\n",
      " [0.04196933 0.57990315]]\n",
      "Running model: vars_allPMTmin_outliers_30\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "R2 test: 0.46625208854675293\n",
      "R2 train: 0.6012842655181885\n",
      "Confusion Matrix: \n",
      "[[0.2720619  0.11522376]\n",
      " [0.11522376 0.49749059]]\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Accuracy test: 0.7252195734002509\n",
      "Accuracy train: 0.7305427199420133\n",
      "Confusion Matrix: \n",
      "[[0.17273107 0.21894605]\n",
      " [0.05583438 0.5524885 ]]\n"
     ]
    }
   ],
   "source": [
    "def fit_xgboost_reg(df, y_var, vars, params_grid, scoring=\"neg_mean_absolute_error\"):\n",
    "    \"\"\"\n",
    "    Trains and evaluates an XGBoost regression model using a randomized grid search for hyperparameter tuning.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the dataset with both training and test sets, including features and target variables.\n",
    "        The DataFrame should have a column 'test_set' where 0 indicates training data and 1 indicates test data.\n",
    "    \n",
    "    y_var: str\n",
    "        A column name representing the variable to use as labels of the model.\n",
    "\n",
    "    vars : list\n",
    "        A list of column names representing the independent variables (features) used for training the model.\n",
    "        \n",
    "    params_grid : dict\n",
    "        Dictionary where keys are XGBoost hyperparameters and values are lists of possible values for those\n",
    "        hyperparameters. This grid is used for randomized hyperparameter tuning.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    random_search : RandomizedSearchCV object\n",
    "        The fitted RandomizedSearchCV object containing the best estimator and results from the grid search.\n",
    "        \n",
    "    Workflow:\n",
    "    ---------\n",
    "    1. Splits the input DataFrame into training and test sets based on the 'test_set' column.\n",
    "    2. Prepares the feature matrix (X) and target variable (y) for both training and test sets.\n",
    "    3. Converts the data into DMatrix format required for XGBoost.\n",
    "    4. Initializes an XGBoost regressor with the 'hist' tree method and 'cuda' for GPU acceleration.\n",
    "    5. Conducts a RandomizedSearchCV to tune hyperparameters based on a given parameter grid.\n",
    "    6. Fits the model to the training data and evaluates its performance on both the training and test sets.\n",
    "    7. Prints R-squared metrics for both training and test sets.\n",
    "    8. Returns the fitted RandomizedSearchCV object.\n",
    "    \"\"\"\n",
    "\n",
    "    df_test = df[df.test_set==1]\n",
    "    df_train = df[df.test_set==0]\n",
    "    X_test = df_test[vars]\n",
    "    y_test = df_test[y_var]\n",
    "    X_train = df_train[vars]\n",
    "    y_train = df_train[y_var]\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model, \n",
    "        param_distributions=params_grid, \n",
    "        n_iter=200, \n",
    "        cv=5, \n",
    "        scoring='neg_mean_absolute_error', \n",
    "        verbose=10, \n",
    "        n_jobs=4,\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    y_test_preds = random_search.best_estimator_.predict(X_test)\n",
    "    y_train_preds = random_search.best_estimator_.predict(X_train)\n",
    "\n",
    "    r_test = r2_score(y_test, y_test_preds, multioutput='uniform_average')\n",
    "    r_train = r2_score(y_train, y_train_preds, multioutput='uniform_average')\n",
    "    print(f\"R2 test: {r_test}\")\n",
    "    print(f\"R2 train: {r_train}\")\n",
    "\n",
    "    cm = compute_confusion_matrix(y_test, y_test_preds, percentage=0.6127813) # Poverty Headcount ratio\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(cm)\n",
    "\n",
    "    return random_search, y_test_preds, r_test, r_train, cm\n",
    "\n",
    "def fit_xgboost_cla(df, y_var, vars, params_grid, scoring=\"f1\"):\n",
    "    \"\"\"\n",
    "    Trains and evaluates an XGBoost regression model using a randomized grid search for hyperparameter tuning.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the dataset with both training and test sets, including features and target variables.\n",
    "        The DataFrame should have a column 'test_set' where 0 indicates training data and 1 indicates test data.\n",
    "    \n",
    "    y_var: str\n",
    "        A column name representing the variable to use as labels of the model.\n",
    "\n",
    "    vars : list\n",
    "        A list of column names representing the independent variables (features) used for training the model.\n",
    "        \n",
    "    params_grid : dict\n",
    "        Dictionary where keys are XGBoost hyperparameters and values are lists of possible values for those\n",
    "        hyperparameters. This grid is used for randomized hyperparameter tuning.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    random_search : RandomizedSearchCV object\n",
    "        The fitted RandomizedSearchCV object containing the best estimator and results from the grid search.\n",
    "        \n",
    "    Workflow:\n",
    "    ---------\n",
    "    1. Splits the input DataFrame into training and test sets based on the 'test_set' column.\n",
    "    2. Prepares the feature matrix (X) and target variable (y) for both training and test sets.\n",
    "    3. Converts the data into DMatrix format required for XGBoost.\n",
    "    4. Initializes an XGBoost regressor with the 'hist' tree method and 'cuda' for GPU acceleration.\n",
    "    5. Conducts a RandomizedSearchCV to tune hyperparameters based on a given parameter grid.\n",
    "    6. Fits the model to the training data and evaluates its performance on both the training and test sets.\n",
    "    7. Prints R-squared metrics for both training and test sets.\n",
    "    8. Returns the fitted RandomizedSearchCV object.\n",
    "    \"\"\"\n",
    "\n",
    "    df_test = df[df.test_set==1]\n",
    "    df_train = df[df.test_set==0]\n",
    "    X_test = df_test[vars]\n",
    "    y_test = df_test[y_var]\n",
    "    X_train = df_train[vars]\n",
    "    y_train = df_train[y_var]\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model, \n",
    "        param_distributions=params_grid, \n",
    "        n_iter=200, \n",
    "        cv=5, \n",
    "        scoring=scoring, \n",
    "        verbose=10, \n",
    "        n_jobs=4,\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    y_test_preds = random_search.best_estimator_.predict(X_test).round()\n",
    "    y_train_preds = random_search.best_estimator_.predict(X_train).round()\n",
    "    \n",
    "    a_test = accuracy_score(y_test, y_test_preds)\n",
    "    a_train = accuracy_score(y_train, y_train_preds)\n",
    "    print(f\"Accuracy test: {a_test}\")\n",
    "    print(f\"Accuracy train: {a_train}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_test_preds, normalize=\"all\")\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(cm)\n",
    "\n",
    "    return random_search, y_test_preds, a_test, a_train, cm\n",
    "\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(y_test, y_test_preds, percentage=.3985):\n",
    "    threshold = np.quantile(y_test.values, percentage)\n",
    "    real_classification = (y_test.values < threshold)\n",
    "\n",
    "    threshold = np.quantile(y_test_preds, percentage)\n",
    "    preds_classification = (y_test_preds < threshold)\n",
    "\n",
    "    return confusion_matrix(real_classification, preds_classification, normalize=\"all\")\n",
    "\n",
    "\n",
    "params_grid = {\n",
    "    'learning_rate': stats.uniform(0.005, 0.1),\n",
    "    'reg_alpha': stats.uniform(0, 0.1),\n",
    "    'reg_lambda': stats.uniform(0.7, 0.3),\n",
    "    'gamma':stats.uniform(0.1, 0.5),\n",
    "    'max_depth': stats.randint(5, 15),\n",
    "    'min_child_weight': stats.randint(5, 20),\n",
    "    'n_estimators': stats.randint(100, 200),\n",
    "    'colsample_bytree':stats.uniform(0.5, 0.5),\n",
    "    'subsample': stats.uniform(0.5, .25),\n",
    "}\n",
    "\n",
    "casos = {\n",
    "    \"vars_all_outliers_min\": {\n",
    "        \"df\":df, \n",
    "        \"vars\": vars_PMT_nonoise + vars_IPM,\n",
    "    },\n",
    "    \"vars_all_outliers_all\": {\n",
    "        \"df\":df, \n",
    "        \"vars\": vars_PMT_nonoise + vars_IPM + vars_PMT_noise + vars_Nico,\n",
    "    },\n",
    "    \"vars_all_outliers_30\": {\n",
    "        \"df\":df[df.outliers_30 == 0], \n",
    "        \"vars\": vars_PMT_nonoise + vars_IPM + vars_PMT_noise + vars_Nico,\n",
    "    },\n",
    "    \"vars_allPMT_outliers_all\": {\n",
    "        \"df\":df, \n",
    "        \"vars\": vars_PMT_nonoise + vars_PMT_noise,\n",
    "    },\n",
    "    \"vars_allPMT_outliers_30\": {\n",
    "        \"df\":df[df.outliers_30 == 0], \n",
    "        \"vars\": vars_PMT_nonoise + vars_PMT_noise,\n",
    "    },\n",
    "    \"vars_allPMT-IPM_outliers_all\": {\n",
    "        \"df\":df, \n",
    "        \"vars\": vars_PMT_nonoise + vars_PMT_noise + vars_IPM,\n",
    "    },\n",
    "    \"vars_allPMT-IPM_outliers_30\": {\n",
    "        \"df\":df[df.outliers_30 == 0], \n",
    "        \"vars\": vars_PMT_nonoise + vars_PMT_noise + vars_IPM,\n",
    "    },\n",
    "    \"vars_allPMTmin_outliers_all\": {\n",
    "        \"df\":df, \n",
    "        \"vars\": vars_PMT_nonoise,\n",
    "    },\n",
    "    \"vars_allPMTmin_outliers_30\": {\n",
    "        \"df\":df[df.outliers_30 == 0], \n",
    "        \"vars\": vars_PMT_nonoise,\n",
    "    }\n",
    "}\n",
    "\n",
    "resultados = {}\n",
    "for name, params in casos.items():\n",
    "    print(f\"Running model: {name}\")\n",
    "    df_model, vars = params.values()\n",
    "    results, y_test_preds, r_test, r_train, cm = fit_xgboost_reg(df_model, \"logingreso\", vars, params_grid)\n",
    "    resultados[name + \"_reg\"] = {}\n",
    "    resultados[name + \"_reg\"][\"df\"] = df_model\n",
    "    resultados[name + \"_reg\"][\"results\"] = results\n",
    "    resultados[name + \"_reg\"][\"best_params\"] = results.best_params_\n",
    "    resultados[name + \"_reg\"][\"r_test\"]  = r_test\n",
    "    resultados[name + \"_reg\"][\"r_train\"] = r_train\n",
    "    resultados[name + \"_reg\"][\"confusion_matrix\"] = cm\n",
    "    resultados[name + \"_reg\"][\"y_test_preds\"] = y_test_preds\n",
    "\n",
    "    results, y_test_preds, r_test, r_train, cm = fit_xgboost_cla(df_model, \"pobreza\", vars, params_grid)\n",
    "    resultados[name + \"_cla\"] = {}\n",
    "    resultados[name + \"_cla\"][\"df\"] = df_model\n",
    "    resultados[name + \"_cla\"][\"results\"] = results\n",
    "    resultados[name + \"_cla\"][\"best_params\"] = results.best_params_\n",
    "    resultados[name + \"_cla\"][\"r_test\"]  = r_test\n",
    "    resultados[name + \"_cla\"][\"r_train\"] = r_train\n",
    "    resultados[name + \"_cla\"][\"confusion_matrix\"] = cm\n",
    "    resultados[name + \"_cla\"][\"y_test_preds\"] = y_test_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resultados[\"vars_all_outliers_min_reg\"][\"results\"]\n",
    "df[\"logingreso_xgboost\"] = model.best_estimator_.predict(df[vars_PMT_nonoise + vars_IPM])\n",
    "df[[\"HOGAR\",\"YPERHG\",\"TOTPER\",\"FACTOR\", \"UR\",\"logingreso_xgboost\", \"indice_pobreza_multi\", \"pobreza\", \"pobreza_ext\"]].set_index(\"HOGAR\").to_stata(r\"D:\\World Bank\\Honduras PMT benchmark\\Data_out\\Predicts_XGBoost.dta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"log_ingreso_est_PMT_carlos\"] = 7.0556 + 0.1236 * df[\"Basura_bien\"] + 0.4290 * df[\"Vivienda2_bien\"] + 0.1013 * df[\"Paredes_bien\"] + 0.2017 * df[\"Alumbrado_bien\"] + 0.1126 * df[\"Cocina2_bien\"] - 0.1565 * df[\"Refri_mal\"] - 0.0963 * df[\"Estufa_mal\"] - 0.1373 * df[\"Carro_mal\"] - 0.1725 * df[\"Compu_mal\"] - 0.1585 * df[\"Aire_mal\"] + 0.1474 * df[\"Ed_diversif_bien\"] + 0.3999 * df[\"Ed_univer_bien\"] + 0.2402 * df[\"Ocupacion_bien\"] + 0.9922 * df[\"Dependencia\"]+ 0.3570 * df[\"Pension_bien\"] + 0.3503 * df[\"Alquileres_bien\"] + 0.1530 * df[\"Remesas_bien\"] - 0.0712 * df[\"edad_0_5\"] - 0.1111 * df[\"edad_6_14\"] - 0.0415 * df[\"edad_15_21\"] - 0.0193 * df[\"edad_60_120\"] + 0.0438 * df[\"dv111\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_2356\\2088539463.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"asignado_test\"] = (df[\"YPERHG\"] > df[\"YPERHG\"].quantile(0.6127813))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vars_all_outliers_min_reg\n",
      "vars_all_outliers_min_cla\n",
      "vars_all_outliers_all_reg\n",
      "vars_all_outliers_all_cla\n",
      "vars_all_outliers_30_reg\n",
      "vars_all_outliers_30_cla\n",
      "vars_allPMT_outliers_all_reg\n",
      "vars_allPMT_outliers_all_cla\n",
      "vars_allPMT_outliers_30_reg\n",
      "vars_allPMT_outliers_30_cla\n",
      "vars_allPMT-IPM_outliers_all_reg\n",
      "vars_allPMT-IPM_outliers_all_cla\n",
      "vars_allPMT-IPM_outliers_30_reg\n",
      "vars_allPMT-IPM_outliers_30_cla\n",
      "vars_allPMTmin_outliers_all_reg\n",
      "vars_allPMTmin_outliers_all_cla\n",
      "vars_allPMTmin_outliers_30_reg\n",
      "vars_allPMTmin_outliers_30_cla\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "\n",
    "scores = {}\n",
    "\n",
    "df_test = df[df.test_set==1]\n",
    "df_test[\"asignado_test\"] = (df[\"YPERHG\"] > df[\"YPERHG\"].quantile(0.6127813))\n",
    "\n",
    "# IPM\n",
    "scores[\"IPM\"] = {}\n",
    "accuracy = accuracy_score(y_true=df_test[\"asignado_test\"], y_pred=df_test[\"pobreza_multidim\"])\n",
    "f2 = fbeta_score(y_true=df_test[\"asignado_test\"], y_pred=df_test[\"pobreza_multidim\"], beta=2)\n",
    "scores[\"IPM\"][\"accuracy_score\"] = accuracy\n",
    "scores[\"IPM\"][\"f2\"] = f2\n",
    "\n",
    "# PMT Carlos\n",
    "scores[\"PMT_carlos\"] = {}\n",
    "threshold = np.quantile(df_test[\"log_ingreso_est_PMT_carlos\"].values, 0.6127813)\n",
    "PMT_classification = (df_test[\"log_ingreso_est_PMT_carlos\"].values < threshold)\n",
    "\n",
    "accuracy = accuracy_score(y_true=df_test[\"asignado_test\"].values, y_pred=PMT_classification)\n",
    "f2 = fbeta_score(y_true=df_test[\"asignado_test\"].values, y_pred=PMT_classification, beta=2)\n",
    "scores[\"PMT_carlos\"][\"accuracy_score\"] = accuracy\n",
    "scores[\"PMT_carlos\"][\"f2\"] = f2\n",
    "\n",
    "\n",
    "for name, caso in resultados.items():\n",
    "    print(name)\n",
    "    scores[name] = {}\n",
    "    df_caso = casos[name[:-4]][\"df\"]\n",
    "    y_test = df_caso[df_caso.test_set == 1].logingreso\n",
    "    \n",
    "    threshold = np.quantile(y_test.values, 0.6127813)\n",
    "    real_classification = (y_test.values < threshold)\n",
    "\n",
    "    y_test_preds = caso[\"y_test_preds\"]\n",
    "    if y_test_preds.dtype == \"float32\":\n",
    "        threshold = np.quantile(y_test_preds, 0.6127813)\n",
    "        preds_classification = (y_test_preds < threshold)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=real_classification, y_pred=preds_classification)\n",
    "    f2 = fbeta_score(y_true=real_classification, y_pred=preds_classification, beta=2)\n",
    "    scores[name][\"accuracy_score\"] = accuracy\n",
    "    scores[name][\"f2\"] = f2\n",
    "\n",
    "\n",
    "df = pd.DataFrame().from_dict(scores, orient=\"index\")\n",
    "df.to_excel(r\"D:\\World Bank\\Honduras PMT benchmark\\Outputs\\resultados modelos boosting.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
